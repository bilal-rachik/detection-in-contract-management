{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "detectron.ipynb",
      "provenance": [],
      "private_outputs": true,
      "mount_file_id": "18OePi0e69EyvyLCHo9YE-oE1N_F2OM8h",
      "authorship_tag": "ABX9TyMS4m0kBFyRJ69awu1X5EC3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bilal-rachik/detection-in-contract-management/blob/master/detectron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-Ry3bwZ0g1U",
        "colab_type": "text"
      },
      "source": [
        "# Handwriting,signatures and initials detection in contract management __Part 3 __ Detectron2.\n",
        "Dans l'article précédent, nous avons formé notre modèle de détection des écritures manuscrites, signatures , intials  à l'aide de l'API de détection d'objets de TensorFlow.\n",
        "\n",
        "Dnas cet article nous allons découvrire un autre framework  pour la construction des modèles de détection d'objets et de segmentation d'images, qui s'appelle Detectron2.\n",
        "\n",
        "\n",
        "Dans cet article, nous allons découvrir un autre cadre pour la construction de modèles de détection d'objets et de segmentation d'images, qui s'appelle Detectron2.\n",
        "\n",
        "Parallèlement à la sortie de la version 1.3 de PyTorch, Facebook a également publié une réécriture fondamentale de son cadre de détection d'objets Detectron . Le nouveau framework s'appelle Detectron2 et est maintenant implémenté dans PyTorch .\n",
        "\n",
        "Detectron2 nous permet facilement de construire des modèles de détection d'objets. Cet article vous aidera à démarrer avec Detectron2 en apprenant à  former votre propre modèle.\n",
        "\n",
        "\n",
        "# Installer Detectron2\n",
        "L'installation de Detectron2 est facile par rapport à d'autres frameworks de détection d'objets comme l'API de détection d'objets Tensorflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9yEed6X2bCD",
        "colab_type": "text"
      },
      "source": [
        "# Installer Detectron2\n",
        "L'installation de Detectron2 est facile par rapport à d'autres frameworks de détection d'objets comme l'API de détection d'objets Tensorflow.\n",
        "\n",
        "# Installation sur Google Colab\n",
        "Si vous travaillez dans Google Colab, il peut être installé avec les quatre lignes suivantes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5avrHR_-0spd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# install dependencies: (use cu100 because colab is on CUDA 10.0)\n",
        "!pip install -U torch==1.4+cu100 torchvision==0.5+cu100 -f https://download.pytorch.org/whl/torch_stable.html \n",
        "!pip install cython pyyaml==5.1\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "import torch, torchvision\n",
        "torch.__version__\n",
        "!gcc --version\n",
        "# opencv is pre-installed on colab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKTCo9L_k-yH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install detectron2:\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu100/index.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6NThJLZ1UGZ",
        "colab_type": "text"
      },
      "source": [
        "# Installation sur une machine locale\n",
        "Si vous travaillez sur une machine locale, ce n'est pas aussi simple que cela, mais toujours gérable.\n",
        "\n",
        "je vous invite à  consulter le guide [d'installation officiel](https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md#common-installation-issues) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73CnCshPA9vm",
        "colab_type": "text"
      },
      "source": [
        "Annoter les images\n",
        "C'est déja fait et bien expliqué  dans mes [article precedent](https://github.com/bilal-rachik/detection-in-contract-management/blob/master/Handwriting%2Csignatures%20and%20initials%20%20detection%20in%20contract%20management%20%20part%201.ipynb) . \n",
        "\n",
        "Nous avons utilisé localement le logiciel open source VOTT développé par Microsoft, pour annoter les contrats.\n",
        "\n",
        "Notre ensemble de données est composé d'une variété de contrats du gouvernement télécharger depuis ce [lien]( https://www.gsa.gov/real-estate/real-estate-services/leasing-policy-procedures/lease-documents)  .\n",
        "\n",
        "L'utilisation de VOTT nous a permis de produire un ensemble de formation de 300  images étiquetées à partir d'un échantillon de contrats du gouvernement en quelques heures. Nous avons tiré notre ensemble de tests de 90 images contractuelles supplémentaires.\n",
        "\n",
        "Une fois que vous avez fini d'annoter votre jeu de données d'image, exportez vos actifs dans VOTT au format Pascal VOC.\n",
        "Structure d'ensemble de données attendue pour Pascal VOC est la suivante :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp1zDEblEb3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pascalVOC-export/\n",
        "  Annotations/\n",
        "  ImageSets/\n",
        "    Main/\n",
        "      train.txt\n",
        "      test.txt\n",
        "      # train.txt or val.txt, if you use these splits\n",
        "  JPEGImages/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOxYGI3mFI1X",
        "colab_type": "text"
      },
      "source": [
        "* annotations: Le dossier contient des fichiers XML, un fichier par image. Il stocke des métadonnées sur une image comme un dossier où l'image est stockée, son nom de fichier, sa taille et chaque boîte englobante...etc\n",
        "* Ensuite, nous avons un JPEGImages dossier, il contient tous nos images jpg.\n",
        "* train.txt et test.txt : est notre ensemble de données divisé pour former et tester le modèle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJA3NgRaGLt2",
        "colab_type": "text"
      },
      "source": [
        "# Enregistrer le jeu de données\n",
        "\n",
        "Pour que Detectron2 sache comment obtenir l'ensemble de données, nous devons l'enregistrer et éventuellement enregistrer des métadonnées pour votre ensemble de données.\n",
        "Le processus est bien décrit avec des détails dans la [documentation Detectron2](https://detectron2.readthedocs.io/tutorials/datasets.html) .\n",
        "\n",
        "En général, Detectron2 utilise son propre format pour la représentation des données qui est similaire aux annotations JSON de COCO. Il s'agit d'implémenter une fonction qui renvoie toutes les informations nécessaires sur les données sous forme de liste et en transmettant le résultat à DatasetCatalog.register\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWzLA6yRHWFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dicts():\n",
        "  ...\n",
        "  return list[dict] in the following format\n",
        "\n",
        "from detectron2.data import DatasetCatalog\n",
        "DatasetCatalog.register(\"my_dataset\", get_dicts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I-UEVeRISf9",
        "colab_type": "text"
      },
      "source": [
        "Pour l'ensemble de données qui est déjà au format COCO, Detectron2 fournit la `register_coco_instances` fonction qui `load_coco_json` vous enregistrera et ajoutera des métadonnées sur votre ensemble de données.\n",
        "Les métadonnées sont un mappage de valeurs-clés qui fournit des informations sur l'ensemble de données comme les noms des classes, les couleurs des classes, la racine des fichiers, etc. qui sont accessibles via `MetadataCatalog.get(dataset_name).some_metadata.`\n",
        "\n",
        "qui pourrait être une inspiration pour nous.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAlPiXgqI-De",
        "colab_type": "text"
      },
      "source": [
        "Dans notre cas, nous avons l'ensemble de données au format Pascal VOC et il n'y a pas de chargeur à usage général pour ce format. Heureusement, Detectron2 a une implémentation pour l'enregistrement des jeux de données Pascal VOC (voir [detectron2/detectron2/data/datasets/pascal_voc.py](https://github.com/facebookresearch/detectron2/blob/master/detectron2/data/datasets/pascal_voc.py) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl61evTvKJ9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "from fvcore.common.file_io import PathManager\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "from detectron2.structures import BoxMode\n",
        "\n",
        "# fmt: off\n",
        "CLASS_NAMES= ['signature','Handwrit','intial']\n",
        "# fmt: on\n",
        "\n",
        "def load_voc_instances(dirname: str, split: str):\n",
        "    \"\"\"\n",
        "    Load Pascal VOC detection annotations to Detectron2 format.\n",
        "    Args:\n",
        "        dirname: Contain \"Annotations\", \"ImageSets\", \"JPEGImages\"\n",
        "        split (str): one of \"train\", \"test\", \"val\", \"trainval\"\n",
        "    \"\"\"\n",
        "    with PathManager.open(os.path.join(dirname, \"ImageSets\", \"Main\", split + \".txt\")) as f:\n",
        "        fileids = np.loadtxt(f, dtype=np.str)\n",
        "\n",
        "    # Needs to read many small annotation files. Makes sense at local\n",
        "    annotation_dirname = PathManager.get_local_path(os.path.join(dirname, \"Annotations/\"))\n",
        "    dicts = []\n",
        "    for fileid in fileids:\n",
        "        anno_file = os.path.join(annotation_dirname, fileid + \".xml\")\n",
        "        jpeg_file = os.path.join(dirname, \"JPEGImages\", fileid + \".jpg\")\n",
        "\n",
        "        with PathManager.open(anno_file) as f:\n",
        "            tree = ET.parse(f)\n",
        "\n",
        "        r = {\n",
        "            \"file_name\": jpeg_file,\n",
        "            \"image_id\": fileid,\n",
        "            \"height\": int(tree.findall(\"./size/height\")[0].text),\n",
        "            \"width\": int(tree.findall(\"./size/width\")[0].text),\n",
        "        }\n",
        "        instances = []\n",
        "\n",
        "        for obj in tree.findall(\"object\"):\n",
        "            cls = obj.find(\"name\").text\n",
        "            # We include \"difficult\" samples in training.\n",
        "            # Based on limited experiments, they don't hurt accuracy.\n",
        "            # difficult = int(obj.find(\"difficult\").text)\n",
        "            # if difficult == 1:\n",
        "            # continue\n",
        "            bbox = obj.find(\"bndbox\")\n",
        "            bbox = [float(bbox.find(x).text) for x in [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]]\n",
        "            # Original annotations are integers in the range [1, W or H]\n",
        "            # Assuming they mean 1-based pixel indices (inclusive),\n",
        "            # a box with annotation (xmin=1, xmax=W) covers the whole image.\n",
        "            # In coordinate space this is represented by (xmin=0, xmax=W)\n",
        "            bbox[0] -= 1.0\n",
        "            bbox[1] -= 1.0\n",
        "            instances.append(\n",
        "                {\"category_id\": CLASS_NAMES.index(cls), \"bbox\": bbox, \"bbox_mode\": BoxMode.XYXY_ABS}\n",
        "            )\n",
        "        r[\"annotations\"] = instances\n",
        "        dicts.append(r)\n",
        "    return dicts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cu3UEpxKeHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_name='sig-hand_int_'\n",
        "dirname ='/content/drive/My Drive/my-project/detectron/PascalVOC-export' \n",
        "\n",
        "for split in [\"train\", \"val\"]:\n",
        "  DatasetCatalog.register(dataset_name+split,lambda: load_voc_instances(dirname, split))\n",
        "  MetadataCatalog.get(dataset_name+split).set(thing_classes=CLASS_NAMES,dirname=dirname,split=split)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SttlZj_zM1tY",
        "colab_type": "text"
      },
      "source": [
        "Vous pouvez basculer vers le jeu de données de test avec option --split test\n",
        "\n",
        "Pour vérifier que le chargement des données est correct, visualisons les annotations d'échantillons sélectionnés au hasard dans l'ensemble d'apprentissage:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueiUlUPCNWC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import random\n",
        "import cv2\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "samples=5\n",
        "split ='train'\n",
        "\n",
        "dataset_dicts = DatasetCatalog.get(dataset_name+split)\n",
        "for d in random.sample(dataset_dicts, samples):\n",
        "        img = cv2.imread(d[\"file_name\"])\n",
        "        visualizer = Visualizer(img[:, :, ::-1],\n",
        "                                metadata=MetadataCatalog.get(dataset_name+split),\n",
        "                                scale=0.5)\n",
        "        vis = visualizer.draw_dataset_dict(d)\n",
        "        cv2_imshow(vis.get_image()[:, :, ::-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_rGEZT1Q5O6",
        "colab_type": "text"
      },
      "source": [
        "Nous sommes prêts à former notre modèle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph3fLYbn_NbE",
        "colab_type": "text"
      },
      "source": [
        "# Former un modèle personnalisé\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0DQlmya0fP3",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FOJXoY2Putz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = ('sig-hand_int_train',)\n",
        "cfg.DATASETS.TEST = ()   # no metrics implemented for this dataset\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\")\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.MAX_ITER = 1000\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg) \n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0jpGuOyh52q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cfg.OUTPUT_DIR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw9B3w-eScEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd output/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT1znuo_icA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}